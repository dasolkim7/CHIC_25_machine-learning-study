# K-최근접 이웃 알고리즘 (k-Nearest Neighbors, KNN)

## 1. KNN의 개념

- 데이터를 분류하거나 예측할 때 사용하는 **비지도 학습** 기반의 알고리즘.
- 아이디어:
  - "새로운 데이터가 들어오면, 주변에서 가장 가까운 K개의 데이터를 살펴보고 다수결(또는 평균)로 결정한다."

---

## 2. KNN의 작동 원리

### 1단계: 데이터 준비

- 학습 데이터(훈련 데이터)와 레이블 필요.
- 예: 데이터 포인트가 어떤 클래스에 속하는지 알려주는 정보.

### 2단계: 거리 계산

- 새로운 데이터와 학습 데이터 간의 **거리를 계산**.
- 거리 계산 방식:
  - **유클리드 거리** (가장 흔함):  
    \[
    d(p, q) = \sqrt{\sum\_{i=1}^{n} (p_i - q_i)^2}
    \]
  - **맨해튼 거리** 또는 **코사인 유사도** 등도 사용 가능.

### 3단계: K개의 최근접 이웃 선택

- 계산된 거리 값을 기준으로 **가까운 순서로 정렬**.
- 상위 K개의 데이터를 선택.

### 4단계: 다수결 또는 평균으로 결정

- **분류 문제**: K개의 이웃 중 가장 많이 속한 클래스를 선택.
- **회귀 문제**: K개의 이웃 값의 평균을 계산.

---

## 3. KNN의 특징

### 장점

- 이해하기 쉽고 구현 간단.
- 훈련 과정이 거의 필요 없음.

### 단점

- 데이터가 많아질수록 계산 비용 증가 (**거리 계산 부담**).
- K 값 설정에 따라 성능이 달라짐.
- 이상치(outlier)에 민감.

---

## 4. K 선택 팁

- K가 너무 작으면: **과적합** 가능성.
- K가 너무 크면: 과도한 일반화로 성능 저하.
- 일반적으로 **홀수**를 선택해 다수결에서 동률 방지.

---

## 5. KNN의 응용

- 추천 시스템
- 이미지 분류
- 패턴 인식
- 이상 탐지 등

---

# K-최근접 이웃 (KNeighborsClassifier) 주요 속성과 메서드

## 주요 속성 6개

1. **`kn.classes_`**

   - 학습 데이터에서 찾은 클래스 레이블(타겟 값)의 목록.
   - 예: `[0, 1]` 또는 `['cat', 'dog']`.

2. **`kn.effective_metric_`**

   - 사용된 거리 측정 기준.
   - 기본값: 유클리드 거리 (Euclidean Distance).
   - 예: `"euclidean"` 또는 `"manhattan"`.

3. **`kn.n_features_in_`**

   - 학습 데이터의 특성(feature) 개수.
   - 예: 2 (특성 개수가 2개라면).

4. **`kn.n_samples_fit_`**

   - 학습에 사용된 샘플(데이터 포인트)의 개수.
   - 예: 100 (데이터 포인트가 100개라면).

5. **`kn._fit_X`**

   - 학습 데이터의 입력값(`X`).
   - 내부적으로 학습 데이터를 저장하며, 비공개 속성으로 `_fit_X`를 통해 접근 가능.

6. **`kn._y`**
   - 학습 데이터의 타겟값(`y`).
   - 비공개 속성으로 학습된 타겟 레이블을 저장.

---

## 주요 메서드 7개

1. kn.fit(X, y)

   - 모델을 학습시킴.
   - 입력 데이터 `X`(특성)와 `y`(레이블)를 사용.

   ```python
   kn.fit(X_train, y_train)
   ```

2. kn.predict(X)
   • 새로운 데이터 X에 대한 예측 결과를 반환.
   • 분류 문제: 각 데이터 포인트의 클래스 레이블.
   • 회귀 문제: 각 데이터 포인트의 예측 값(평균).

```python
  predictions = kn.predict(X_test)
```

    3.	kn.predict_proba(X)
    •	분류 문제에서 각 클래스에 대한 확률을 반환.
    •	예: 클래스 0의 확률이 0.8, 클래스 1의 확률이 0.2.
    ```python

predictions = kn.predict(X_test)

````
	4.	kn.score(X, y)
	•	모델의 성능을 평가하여 정확도를 반환.
	•	입력 데이터 X와 실제 레이블 y가 필요.

```python
  accuracy = kn.score(X_test, y_test)
````

    5.	kn.kneighbors(X, n_neighbors=None, return_distance=True)
    •	입력 데이터 X의 이웃을 계산.
    •	반환값:
    •	거리(return_distance=True일 경우).
    •	인덱스(return_distance=False일 경우).

```python
  distances, indices = kn.kneighbors(X_test)
```

    6.	kn.get_params()
    •	모델의 하이퍼파라미터를 딕셔너리 형태로 반환.

```python
  params = kn.get_params()
```

    7.	kn.set_params(**params)
    •	모델의 하이퍼파라미터를 설정하거나 업데이트.

```python
 kn.set_params(n_neighbors=5, metric='manhattan')
```

## Question

### 1. 데이터가 많아져서 k-최근접 이웃 알고리즘을 사용하는 것이 비효율적이 되는 시기는 어느 정도인가?

    A. 데이터의 크기와 특성에 따라 다르다.    KNN은 데이터가 N > 10^6 수준으로 커지거나 d > 50 이상으로 차원이 높아질 때 비효율적으로 작동할 가능성이 큼.

이런 경우에는 효율적인 데이터 구조(KD-Tree), 근사 알고리즘, 차원 축소 또는 대체 모델을 고려해야 함.

### 2. KNN이 데이터가 많아질수록, 비용이 증가하는 이유는 무엇인가?

    A. 1. 거리 계산이 모든 데이터 포인트를 대상으로 수행됨
    •	KNN은 새로운 데이터 포인트의 레이블(또는 값을) 예측하기 위해 학습 데이터의 모든 점과의 거리를 계산해야 함.
    •	예를 들어, 학습 데이터가 N개라면:
    •	예측하려는 데이터 한 개에 대해 N번의 거리 계산이 필요.
    •	예측 데이터가 M개라면 총 계산량은 O(M \times N).
    2. 거리 계산의 복잡도
    •	거리 계산 자체도 비용이 듦. 특히, d-차원의 데이터에서는 각 차원의 값을 비교해야 함.
    •	유클리드 거리 계산의 경우, 차원 d에 대해 O(d)의 비용이 발생.
    •	전체 거리 계산의 비용은 O(M \times N \times d).
    •	데이터가 많아질수록 N과 d가 커져 계산량이 급격히 증가.
    3. 데이터 정렬 비용
    •	KNN은 거리 계산 후, 가장 가까운 K개의 데이터를 찾기 위해 정렬을 수행.
    •	정렬의 시간 복잡도는 일반적으로 O(N \log N) (데이터 하나에 대해).
    •	따라서 예측 데이터가 M개라면, 총 정렬 비용은 O(M \times N \log N).
    5. 고차원 데이터일수록 계산이 더 어려움
    •	차원이 높아질수록 거리 계산이 비효율적이 됨 (이를 “차원의 저주(Curse of Dimensionality)“라고 함).
    •	고차원 데이터에서는 모든 데이터가 비슷한 거리로 분포할 가능성이 높아져, 가까운 이웃을 판별하기가 어려워짐.
    •	이는 KNN의 성능뿐만 아니라 계산 비용에도 영향을 줌.
