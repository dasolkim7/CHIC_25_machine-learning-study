~86p

1. 간단한 notations

2. 데이터셋에서 length와 weight 가져와서 묶어 2차원 리스트로 만들기

3. 2차원 리스트에서, bream이 존재하는 인덱스를 1로 하여 target 변수 리스트 만들기

4. k-nearest neighbors << 왜 바로 이걸로 넘어가요..?

   - `sklearn` 라이브러리
   - `KNeighborsClassifier` class
   - `fit()` method
   - `score()` method
   - `predict()` method

5. train set, test set 나누기

   - numpy로 shuffle 후 slicing하여 나누기

---

> q1 <br/>
> 책과 같이 feature variables가 묶인 2차원 리스트 + target variables가 있는 1차원 리스트로 나타내는 게 맞을까요? 애초에 일반적으로 리스트로 관리하긴 하나

- 일반적으로 numpy array나 pandas df 사용

<br/>

> q2 <br/>
> 책에서는 데이터셋 확인을 그래프 하나로만 진행하고 있는데.. 이래도 괜찮을까

- 크기 확인도 필요 -> feature들마다의 크기 차이가 크다면, 편향 발생 가능성 존재하기에 (특히 knn처럼 거리 기반 알고리즘들). 그럴경우, 이런저런 방법(Normalization)들 사용해서 크기를 맞춰주는 작업 추가로 필요.
- 등분산성 확인 필요 -> 책에선 k nearest neighbors사용해서 큰 문제가 없지만, 회귀모델은 등분산성이 기본 가정이기에 [BLUE](https://wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) 만족x

<br/>

> q3 <br/>
> k nearest neighbors가 어케작동하는지 안 알려줬어요. -> 3차원까지는 어케 작동할지 예상 되는데.. 그 이상은 상상도 안돼요.

- 직접 구현해봐야

<br/>

> q4 <br/>
> 고차원이라도 train set과 test set을 slicing으로 나눌 수 있을까요?

- 물론 가능. 물론 데이터가 어떻게 생겼는지는 확실히 인지하고 있어야 함.

- 일반적으론 scikit-learn의 `train_test_split()` 사용. 랜덤으로 섞어주기도 하고, [층화추출](https://wikipedia.org/wiki/Stratified_sampling)도 가능. (df or np array을 사용해야 함.)
